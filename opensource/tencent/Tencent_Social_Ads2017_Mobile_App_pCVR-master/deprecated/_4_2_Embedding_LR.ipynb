{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenweichen/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from feature_joint import addTime,addAd,addPosition,addAppCategories,addUserInfo\n",
    "from _2_0_merge_others_features import add_new_trick,add_hist_cvr_smooth,add_smooth_pos_cvr,add_install2click\n",
    "from _2_0_merge_others_features import add_cate_count,add_interval,add_basic_stats\n",
    "\n",
    "from _2_1_gen_user_day_hour_click_feature import add_user_day_click_count,add_user_day_hour_count,add_user_click_stats,add_user_ratio_per_day,add_user_day_click\n",
    "from _2_2_gen_app_install_feature import add_app_hist_install,add_app_start_installed,add_user_hist_install,add_user_start_installed_cateA\n",
    "from _2_2_gen_app_install_feature import get_ConcatedAppIDTfidfVector_userinstalled\n",
    "\n",
    "from _2_3_gen_IDs_count_per_day import add_IDs_count_per_day\n",
    "from _2_4_gen_data_clk_conv_vector import get_ConcatedTfidfVector_ID_user_clicks,get_ConcatedTfidfVector_ID_user_convs\n",
    "from _2_6_gen_combe_index import add_comb_features\n",
    "from _2_8_gen_global_count import add_global_count_sum\n",
    "from _2_9_gen_comb_cvr import add_comb_cvr\n",
    "\n",
    "from utils import load_pickle,dump_pickle,get_feature_value,feature_spearmanr,feature_target_spearmanr,addCrossFeature,calibration\n",
    "from utils import raw_data_path,trans_data_path,cache_pkl_path\n",
    "from MeanEncoder import MeanEncoder\n",
    "\n",
    "hhy_path = '../hhy/X_v2/'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(clf,train_x,test_x,feature_names,cate_features,mode='offline'):\n",
    "    if mode=='offline':\n",
    "        start_time = time.time()\n",
    "        clf.fit(train_x[feature_names],train_x['label'],\n",
    "               eval_set=[(train_x[feature_names],train_x['label']),(test_x[feature_names],test_x['label'])],\n",
    "                 feature_name=feature_names,categorical_feature=cate_features,\n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose=10,\n",
    "           )\n",
    "        total_time = time.time()-start_time\n",
    "        print('offline training done {0}m{1:.1f}s'.format(total_time//60,total_time%60))\n",
    "        print('best iteration {0}'.format(clf.best_iteration))\n",
    "        print('best score {0:.6f}'.format(clf.best_score['valid_1']['binary_logloss']))\n",
    "        #pred = clf.predict_proba(test_x.loc[:,feature_names],num_iteration=clf.best_iteration)[:,1]\n",
    "        #print('%.7f'%log_loss(test_x.loc[:,'label'],pred))\n",
    "    \n",
    "    elif mode=='online':\n",
    "        start_time = time.time()\n",
    "        clf.fit(train_x[feature_names],train_x['label'],\n",
    "               eval_set=[(train_x[feature_names],train_x['label'])],\n",
    "                 feature_name=feature_names,categorical_feature=cate_features,\n",
    "                    #early_stopping_rounds=20,\n",
    "                    verbose=10,\n",
    "           )\n",
    "        total_time = time.time()-start_time\n",
    "        print('online training done {0}m{1:.1f}s'.format(total_time//60,total_time%60))\n",
    "    return clf\n",
    "\n",
    "def gen_result(clf,test_x,feature_names):\n",
    "    #预测大概需要5分钟\n",
    "    print('start predicting...')\n",
    "    start_time = time.time()\n",
    "    test_prob = clf.predict_proba(test_x[feature_names])[:,1]\n",
    "    print(test_prob.mean())\n",
    "    result = pd.read_csv('../result/demo_result.csv',index_col=['instanceID'])\n",
    "    result['prob'] = test_prob\n",
    "    filename = 'submission_'+'_'.join(time.ctime()[4:16].replace(':',' ').split(' '))+'.zip'\n",
    "    cali_name = 'submission_'+'_'.join(time.ctime()[4:16].replace(':',' ').split(' '))+'.cali.zip'\n",
    "    result.to_csv('submission.csv')\n",
    "    print(filename)\n",
    "    with zipfile.ZipFile('../result/'+filename, \"w\") as fout:\n",
    "        fout.write(\"submission.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "    total_time = time.time() - start_time\n",
    "    print('online predicting time {0}m{1:.1f}s'.format(total_time//60,total_time%60))\n",
    "    return cali_name\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def stratified_sampling(train,frac=0.33,seed=0):\n",
    "    np.random.seed(seed)\n",
    "    label_mean = train['label'].mean()\n",
    "    pos_size = int(train.shape[0]*frac*label_mean)\n",
    "    neg_size = int(train.shape[0]*frac*(1 - label_mean))\n",
    "    pos_index = train[train.label==1].index\n",
    "    neg_index = train[train.label==0].index\n",
    "    sample_pos_idx = np.random.choice(pos_index,pos_size,replace=False)\n",
    "    sample_neg_idx = np.random.choice(neg_index,neg_size,replace=False)\n",
    "    sample_index = np.hstack([sample_pos_idx,sample_neg_idx])\n",
    "    np.random.shuffle(sample_index)\n",
    "    return train.loc[sample_index]\n",
    "\n",
    "def addFeature(data):\n",
    "    \n",
    "    \n",
    "    #pos_cvr = load_pickle('../trans_data_new/offline_pos_cvr.cvr')\n",
    "    #data = pd.merge(data,pos_cvr,'left','positionID')\n",
    "    \n",
    "   \n",
    "    tfidf_appcate_hour = get_TfidfVector_appCategory_user_action_hour()\n",
    "    #data = pd.merge(data,tfidf_appcate_hour,'left','appCategory')\n",
    "    \n",
    "    n_components = 8\n",
    "    svd = TruncatedSVD(n_components=n_components,random_state=0,)\n",
    "    svd_vector = pd.DataFrame(svd.fit_transform(tfidf_appcate_hour.iloc[:,1:]),columns=['svd_appCategory_'+str(i) for i in range(n_components)],\n",
    "                              index=tfidf_appcate_hour.appCategory).reset_index()\n",
    "    #svd.explained_variance_ratio_.sum()\n",
    "    data = pd.merge(data,svd_vector,'left','appCategory')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def addMeanEncoderFeature(split_train,split_test,feature,mode):\n",
    "    train_feature_path = '../feature_pkl//'+mode+'_train_'+feature+'_MeanEncoder_.pkl'\n",
    "    test_feature_path = '../feature_pkl/'+mode+'_test_'+feature+'_MeanEncoder_.pkl'\n",
    "    feature_names = ['_'.join([feature,'pred','0']),'_'.join([feature,'pred','1'])]\n",
    "    if os.path.exists(train_feature_path) and os.path.exists(test_feature_path):\n",
    "        train_me = pickle.load(open(train_feature_path,'rb'))\n",
    "        test_me = pickle.load(open(test_feature_path,'rb'))\n",
    "        split_train = split_train.join(train_me)\n",
    "        split_test = split_test.join(test_me)\n",
    "    else:\n",
    "        \n",
    "        me = MeanEncoder([feature])\n",
    "        split_train = me.fit_transform(split_train, split_train['label'])\n",
    "        print(feature+' ME fit done')\n",
    "        split_test_index = split_test.index\n",
    "        split_test = me.transform(split_test)\n",
    "        split_test.set_index(split_test_index,inplace=True)\n",
    "        #pickle.dump(split_train[feature_names],open(train_feature_path,'wb'))\n",
    "        #pickle.dump(split_test[feature_names],open(test_feature_path,'wb'))\n",
    "                                                             \n",
    "    return split_train,split_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(start_day=23,end_day=26,load_test=False):\n",
    "    \"\"\"\n",
    "    读取基本表拼接后的数据\n",
    "    test表load_test = True\n",
    "    \"\"\"\n",
    "    if load_test ==True:\n",
    "        trans_train_path = trans_data_path+'trans_test_'+str(start_day)+'_'+str(end_day)+'.pkl'\n",
    "        raw_train_path = raw_data_path +'test.pkl'\n",
    "    else:\n",
    "        trans_train_path = trans_data_path+'trans_train_'+str(start_day)+'_'+str(end_day)+'.pkl'\n",
    "        raw_train_path = raw_data_path +'train.pkl'\n",
    "\n",
    "    if os.path.exists(trans_train_path):\n",
    "        print('found '+trans_train_path)\n",
    "        train = pickle.load(open(trans_train_path,'rb'))\n",
    "    else:\n",
    "        print('generating '+trans_train_path)\n",
    "        train = load_pickle(raw_train_path)\n",
    "\n",
    "        train = addTime(train)\n",
    "        train = train[(train.clickDay>=start_day)&(train.clickDay<=end_day)]\n",
    "        train = addAd(train)\n",
    "        train = addPosition(train)\n",
    "        train = addAppCategories(train)\n",
    "        train = addUserInfo(train)\n",
    "             \n",
    "        dump_pickle(train,trans_train_path)\n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_online_data(train_start_day,train_end_day,test_day):\n",
    "    \n",
    "    train_x_path = cache_pkl_path +'online_train_x_'+str(train_start_day)+'_'+str(train_end_day)+'.pkl'\n",
    "    test_x_path = cache_pkl_path + 'online_test_x_'+str(test_day)+'_'+str(test_day)+'.pkl'\n",
    "    \n",
    "    alpha = 0.647975342478\n",
    "    beta = 34.83752176\n",
    "    pos_na = alpha / (alpha + beta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if os.path.exists(train_x_path):\n",
    "        print('found '+train_x_path)\n",
    "        train_x  = load_pickle(train_x_path)\n",
    "    else:\n",
    "        print('generating '+train_x_path)\n",
    "        train_x = load_data(train_start_day,train_end_day,False)\n",
    "        train_x['age_cut']=pd.cut(train_x['age'],bins=[-1,0,18,25,35,45,55,65,np.inf],labels=False)\n",
    "        \n",
    "        #-----------trick-------------------------------\n",
    "        train_x = add_new_trick(train_x,'train')\n",
    "        #-----------intstall和action表相关----------------------\n",
    "        print('adding install and actions...')\n",
    "        train_x = add_user_start_installed_cateA(train_x)\n",
    "        train_x = add_user_hist_install(train_x,'train')\n",
    "        train_x = add_app_hist_install(train_x)\n",
    "        train_x = add_app_start_installed(train_x)\n",
    "         #-----------用户点击相关---------------------------- \n",
    "        print('adding user clicks...')\n",
    "        train_x = add_user_day_click_count(train_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        train_x = add_user_day_hour_count(train_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        train_x = add_user_day_click(train_x)\n",
    "        train_x = add_user_click_stats(train_x,'train')\n",
    "         #------------转化率相关------------------------------\n",
    "        print('adding conversions')\n",
    "        train_x = add_smooth_pos_cvr(train_x,'online')\n",
    "        train_x = train_x.fillna({'positionID_cvr_smooth': pos_na})\n",
    "        train_x = add_hist_cvr_smooth(train_x,'train',cvr_list=[ 'creativeID', 'adID', 'appID','userID'])\n",
    "         #--------------其他------------------------------------\n",
    "        train_x = add_global_count_sum(train_x,31,stats_features=['positionID', 'creativeID', 'appID', 'adID', 'userID'])\n",
    "        \n",
    "        \n",
    "        dump_pickle(train_x,train_x_path)\n",
    "    \n",
    "    if os.path.exists(test_x_path):\n",
    "        print('found '+test_x_path)\n",
    "        test_x  = load_pickle(test_x_path)\n",
    "    else:\n",
    "        print('generating '+test_x_path)\n",
    "        test_x = load_data(test_day,test_day,True)\n",
    "        test_x['age_cut']=pd.cut(test_x['age'],bins=[-1,0,18,25,35,45,55,65,np.inf],labels=False)\n",
    "        \n",
    "        #-----------trick-------------------------------\n",
    "        test_x = add_new_trick(test_x,'test')\n",
    "        #-----------intstall和action表相关----------------------\n",
    "        print('adding install and actions...')\n",
    "        test_x = add_user_start_installed_cateA(test_x)\n",
    "        test_x = add_user_hist_install(test_x,'test')\n",
    "        test_x = add_app_hist_install(test_x)\n",
    "        test_x = add_app_start_installed(test_x)\n",
    "         #-----------用户点击相关---------------------------- \n",
    "        print('adding user clicks...')\n",
    "        test_x = add_user_day_click_count(test_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        test_x = add_user_day_hour_count(test_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        test_x = add_user_day_click(test_x)\n",
    "        test_x = add_user_click_stats(test_x,'test')\n",
    "         #------------转化率相关------------------------------\n",
    "        print('adding conversions')\n",
    "        test_x = add_smooth_pos_cvr(test_x,'online')\n",
    "        test_x = test_x.fillna({'positionID_cvr_smooth': pos_na})\n",
    "        test_x = add_hist_cvr_smooth(test_x,'test',cvr_list=[ 'creativeID', 'adID', 'appID','userID'])\n",
    "         #--------------其他------------------------------------\n",
    "        test_x = add_global_count_sum(test_x,31,stats_features=['positionID', 'creativeID', 'appID', 'adID', 'userID'])\n",
    "       \n",
    "        \n",
    "        dump_pickle(test_x,test_x_path)\n",
    "        \n",
    "    return train_x,test_x\n",
    "        \n",
    "\n",
    "def gen_offline_data(train_start_day,train_end_day,test_day,):\n",
    "    \n",
    "    train_x_path = cache_pkl_path +'offline_train_x_'+str(train_start_day)+'_'+str(train_end_day)+'.pkl'\n",
    "    test_x_path = cache_pkl_path + 'offline_test_x_'+str(test_day)+'_'+str(test_day)+'.pkl'\n",
    "    if os.path.exists(train_x_path) and os.path.exists(test_x_path):\n",
    "        print('found offline data')\n",
    "        train_x = load_pickle(train_x_path)\n",
    "        test_x = load_pickle(test_x_path)\n",
    "    else:\n",
    "        alpha = 0.640792339811\n",
    "        beta = 34.2999347427\n",
    "        pos_na = alpha / (alpha + beta)\n",
    "        print('generating offline data')\n",
    "        train_x = load_data(train_start_day,test_day,False)\n",
    "        train_x['age_cut']=pd.cut(train_x['age'],bins=[-1,0,18,25,35,45,55,65,np.inf],labels=False)\n",
    "            \n",
    "        #-----------trick-------------------------------\n",
    "        train_x = add_new_trick(train_x,'train')\n",
    "        #-----------intstall和action表相关----------------------\n",
    "        print('adding install and actions...')\n",
    "        train_x = add_user_start_installed_cateA(train_x)\n",
    "        train_x = add_user_hist_install(train_x,'train')\n",
    "        train_x = add_app_hist_install(train_x)\n",
    "        train_x = add_app_start_installed(train_x)\n",
    "         #-----------用户点击相关---------------------------- \n",
    "        print('adding user clicks...')\n",
    "        train_x = add_user_day_click_count(train_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        train_x = add_user_day_hour_count(train_x,['camgaignID','adID','sitesetID','appID',])\n",
    "        train_x = add_user_day_click(train_x)\n",
    "        train_x = add_user_click_stats(train_x,'train')\n",
    "         #------------转化率相关------------------------------\n",
    "        print('adding conversions')\n",
    "        alpha = 0.640792339811\n",
    "        beta = 34.2999347427\n",
    "        pos_na = alpha / (alpha + beta)\n",
    "        train_x = add_smooth_pos_cvr(train_x,'offline')\n",
    "        train_x = train_x.fillna({'positionID_cvr_smooth': pos_na})\n",
    "        train_x = add_hist_cvr_smooth(train_x,'train',cvr_list=[ 'creativeID', 'adID', 'appID','userID'])\n",
    "         #--------------其他------------------------------------\n",
    "        train_x = add_global_count_sum(train_x,27,stats_features=['positionID', 'creativeID', 'appID', 'adID', 'userID'])\n",
    "        #------------分割train和test--------------------------\n",
    "        print('splitting train and test ...')\n",
    "        test_x = train_x[train_x.clickDay==test_day]\n",
    "        train_x = train_x[(train_x.clickDay>=train_start_day)&(train_x.clickDay<=train_end_day)]\n",
    "        dump_pickle(train_x,train_x_path,4)\n",
    "        dump_pickle(test_x,test_x_path,4)\n",
    "    return train_x,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found offline data\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x = gen_offline_data(23,26,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_ID_vector(split_train_x,split_test_x,ID_name,last_day,concated_list=['age_cut', 'gender', 'education', 'marriageStatus', 'haveBaby',],mode='local'):\n",
    "    a = get_ConcatedTfidfVector_ID_user_clicks(ID_name,last_day,mode,concated_list=concated_list,drop_na=False)\n",
    "    split_train_x = pd.merge(split_train_x,a,'left',ID_name)\n",
    "    split_test_x = pd.merge(split_test_x,a,'left',ID_name)\n",
    "    return split_train_x,split_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x['age_cut']=pd.cut(train_x['age'],bins=[-1,0,18,25,35,45,55,65,np.inf],labels=False)\n",
    "test_x['age_cut']=pd.cut(test_x['age'],bins=[-1,0,18,25,35,45,55,65,np.inf],labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = ['age_cut','gender', 'education', 'marriageStatus', 'haveBaby','appID','creativeID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = [ 'age','creativeID', 'userID',\n",
    "       'positionID', 'connectionType', 'telecomsOperator', \n",
    "       'adID', 'camgaignID', 'advertiserID', 'appID', 'appPlatform',\n",
    "       'appCategory', 'sitesetID', 'positionType',   \n",
    "       'gender', 'education', 'marriageStatus',\n",
    "       'haveBaby', 'ht_province',  'rd_province',\n",
    "                  'clickHour',\n",
    "       'user_ad_click_day', 'user_camgaign_click_day', 'user_app_click_day',\n",
    "       'user_site_click_day', 'user_click_day', 'user_ad_click_hour',\n",
    "       'user_camgaign_click_hour', 'user_app_click_hour',\n",
    "       'user_site_click_hour', 'installed_cate_0', 'installed_cate_1',\n",
    "       'installed_cate_2', 'installed_cate_3', 'installed_cate_4',\n",
    "       'installed_cate_5'\n",
    "                ]# 'hometown','residence','time'\n",
    "      #'score'\n",
    "cate_features = ['gender', 'education', 'marriageStatus', 'haveBaby','clickHour',\n",
    "                 'ht_province','rd_province','creativeID','adID', \n",
    "        'connectionType', 'telecomsOperator','sitesetID', 'positionType',#'userID',\n",
    "               'camgaignID','appCategory','advertiserID','appPlatform','appID','positionID',\n",
    "              \n",
    "                #  'app_age_audiences',\n",
    "              ]# 'time' \n",
    "\n",
    "\n",
    "try:\n",
    "    feature_names.remove('clickTime')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = train_x.append(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_size_dict ={}\n",
    "for feature in cate_features:\n",
    "    all_data[feature] = lbe.fit_transform(all_data[feature])\n",
    "    id_size_dict[feature] = all_data[feature].max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = all_data[all_data.clickDay<27]\n",
    "test_x = all_data[all_data.clickDay==27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_size_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in cate_features:\n",
    "    id_size_list.append(id_size_dict[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(fetches,sess, X=None, y=None, ):\n",
    "        feed_dict = {}\n",
    "        if type(X) is list:\n",
    "            for i in range(len(X)):\n",
    "                feed_dict[id_inputs[i]] = X[i]\n",
    "                #print(feed_dict[id_inputs[i]])\n",
    "        else:\n",
    "            feed_dict[id_inputs] = X\n",
    "        if y is not None:\n",
    "            feed_dict[label_inputs] = y\n",
    "        return sess.run(fetches, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_feature = cate_features#['appID','gender','connectionType']\n",
    "embedding_size =500\n",
    "id_dtype= tf.int32\n",
    "id_field_num = len(input_feature)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    global_step =  tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "\n",
    "    with tf.name_scope('ID_input'):\n",
    "        id_inputs =[tf.placeholder(id_dtype,shape=[None,]) for i in range(id_field_num)]\n",
    "        label_inputs = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope('Embedding_matrix'):\n",
    "        embeddings = [tf.Variable(tf.ones([id_size_list[i],embedding_size],)) for i in range(0,id_field_num)]\n",
    "        #embedding1 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        #embedding2 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        #embedding3 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        onehot_embeddings = [tf.Variable(tf.random_normal([id_size_list[i],1])) for i in range(0,id_field_num)]\n",
    "        #onehot_embedding1 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "        #onehot_embedding2 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "        #onehot_embedding3 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "    \n",
    "    with tf.name_scope('Embedding_layer'):\n",
    "        embeds = [tf.nn.embedding_lookup(embeddings[i],id_inputs[i]) for i in range(0,id_field_num)]\n",
    "        #embed1 = tf.nn.embedding_lookup(embedding1,appID)\n",
    "        #embed2 = tf.nn.embedding_lookup(embedding2,genderID)\n",
    "        #embed3 = tf.nn.embedding_lookup(embedding3,connectionTypeID)\n",
    "        \n",
    "        onehot_embeds = [tf.nn.embedding_lookup(onehot_embeddings[i],id_inputs[i]) for i in range(0,id_field_num)]\n",
    "        #onehot_embed1 = tf.nn.embedding_lookup(onehot_embedding1,appID)\n",
    "        #onehot_embed2 = tf.nn.embedding_lookup(onehot_embedding2,genderID)\n",
    "        #onehot_embed3 = tf.nn.embedding_lookup(onehot_embedding3,connectionTypeID)\n",
    "    \n",
    "    with tf.name_scope('FM_layer'):\n",
    "        fm_units = []\n",
    "        embed_list=embeds#[embed1,embed2,embed3]\n",
    "        for i in range(0,id_field_num):\n",
    "            for j in range(i+1,id_field_num):\n",
    "                temp = tf.reduce_sum(tf.multiply(embed_list[i],embed_list[j]),axis=1)\n",
    "                fm_units.append(temp)\n",
    "\n",
    "        sum_fm_units =tf.reduce_sum(fm_units,0)\n",
    "        single_onehot_list = tf.concat(onehot_embeds,1)\n",
    "        sum_linear_units = tf.reduce_sum(single_onehot_list,1)\n",
    "    with tf.name_scope('Output'):\n",
    "        logits =  sum_linear_units#tf.add(sum_fm_units,sum_linear_units)\n",
    "        prob = tf.sigmoid(logits)\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_inputs,logits=logits))\n",
    "        batch_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_inputs,logits=logits))\n",
    "        #optimizer = tf.train.AdamOptimizer().minimize(loss,global_step=global_step)\n",
    "        optimizer = tf.train.FtrlOptimizer(0.05).minimize(loss,global_step=global_step)\n",
    "    with tf.name_scope('Summary'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_feature = cate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop_round = 20\n",
    "min_loss = 1e9\n",
    "chance_round = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:59<00:00, 174.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 va-loss 0.1138438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:04<00:00, 160.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 va-loss 0.1120360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:00<00:00, 170.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 va-loss 0.1110141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 178.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 va-loss 0.1103368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 178.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 va-loss 0.1098548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 179.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 va-loss 0.1094922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:59<00:00, 175.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 va-loss 0.1092068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 179.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 va-loss 0.1089750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 179.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 va-loss 0.1087827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:58<00:00, 178.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 va-loss 0.1086207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:58<00:00, 177.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 va-loss 0.1084824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 170.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 va-loss 0.1083628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 180.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 va-loss 0.1082578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:59<00:00, 175.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 va-loss 0.1081647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 179.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 va-loss 0.1080812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:57<00:00, 179.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 va-loss 0.1080055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [00:58<00:00, 175.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 va-loss 0.1079365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:03<00:00, 163.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 va-loss 0.1078730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:01<00:00, 167.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 va-loss 0.1078143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:01<00:00, 169.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 va-loss 0.1077598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:02<00:00, 165.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 va-loss 0.1077089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:01<00:00, 167.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 va-loss 0.1076613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:02<00:00, 166.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 va-loss 0.1076165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10354/10354 [01:02<00:00, 166.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 va-loss 0.1075744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1896/10354 [00:11<00:48, 173.69it/s]"
     ]
    }
   ],
   "source": [
    "train_size = train_x.shape[0]\n",
    "valid_size = test_x.shape[0]\n",
    "batch_size = 1024#10240\n",
    "valid_batch_size = 102400\n",
    "batch_num = train_size//batch_size\n",
    "valid_batch_num = valid_size//valid_batch_size\n",
    "num_round = 1000\n",
    "va_input_X = [test_x[feature].values for feature in input_feature]\n",
    "va_input_y = test_x['label'].values\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./deepFM_log/',graph=sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_round):\n",
    "        for j in tqdm(range(0,1+batch_num)): \n",
    "            X_i = train_x.iloc[j*batch_size:min((j+1)*batch_size,train_size)]\n",
    "            input_X = [X_i[feature].values for feature in input_feature]#X_i[input_feature]\n",
    "            #print(type(input_X.values))\n",
    "            input_y =X_i['label'].values\n",
    "            #_,l,summary = sess.run([optimizer,loss,summary_op],feed_dict={id_inputs:input_X.values,y:input_y})\n",
    "            _,l,summary = run([optimizer,loss,summary_op],sess=sess,X = input_X,y = input_y)\n",
    "            #logit = run([logits],sess=sess,X = input_X,y = input_y)\n",
    "            #print(l)\n",
    "        va_loss_sum = 0 \n",
    "        for j in range(0,1+valid_batch_num):\n",
    "            X_i = test_x.iloc[j*valid_batch_size:min((j+1)*valid_batch_size,valid_size)]\n",
    "            input_X = [X_i[feature].values for feature in input_feature]#X_i[input_feature]\n",
    "            input_y =X_i['label'].values\n",
    "            va_batch_loss = run([batch_loss,],sess=sess,X=input_X,y=input_y,)\n",
    "            #print(va_batch_loss)\n",
    "            va_loss_sum += va_batch_loss[0]\n",
    "        va_loss = va_loss_sum/valid_size\n",
    "        print('epoch {0} va-loss {1:.7f}'.format(i,va_loss))\n",
    "        if va_loss < min_loss:\n",
    "            min_loss = va_loss\n",
    "            chance_round = 0\n",
    "        else:\n",
    "            chance_round +=1\n",
    "        if va_loss < 0.105 or chance_round > early_stop_round:\n",
    "            print('done')\n",
    "            saver.save(sess,'./LR_log/save_model',global_step=global_step)\n",
    "            break\n",
    "        saver.save(sess,'./LR_log/save_model',global_step=global_step)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10603944895273673"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
