{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import coo_matrix\n",
    "from utils import load_pickle\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import load_pickle,dump_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = load_pickle('../raw_data/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.groupby('positionID').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index = b[b<5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.positionID.isin(p_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.groupby(['clickDaY'])['label'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.groupby(['clickDaY'])['label'].size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pickle.load(open(\"../cache_pkl/offline_17_27_train_x.pkl\",'rb'))\n",
    "valid_x = pickle.load(open(\"../cache_pkl/offline_17_27_valid_x.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = ['age_cut','gender', 'education', 'marriageStatus', 'haveBaby','appID','creativeID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [ 'age','creativeID', 'userID',\n",
    "       'positionID', 'connectionType', 'telecomsOperator', \n",
    "       'adID', 'camgaignID', 'advertiserID', 'appID', 'appPlatform',\n",
    "       'appCategory', 'sitesetID', 'positionType',   \n",
    "       'gender', 'education', 'marriageStatus',\n",
    "       'haveBaby', 'ht_province',  'rd_province',\n",
    "                  'clickHour',\n",
    "       'user_ad_click_day', 'user_camgaign_click_day', 'user_app_click_day',\n",
    "       'user_site_click_day', 'user_click_day', 'user_ad_click_hour',\n",
    "       'user_camgaign_click_hour', 'user_app_click_hour',\n",
    "       'user_site_click_hour',# 'installed_cate_0', 'installed_cate_1',\n",
    "      # 'installed_cate_2', 'installed_cate_3', 'installed_cate_4',\n",
    "       #'installed_cate_5'\n",
    "                ]# 'hometown','residence','time'\n",
    "      #'score'\n",
    "cate_features = ['gender', 'education', 'marriageStatus', 'haveBaby','clickHour',\n",
    "                 'ht_province','rd_province','creativeID','adID', \n",
    "        'connectionType', 'telecomsOperator','sitesetID', 'positionType',#'userID',\n",
    "               'camgaignID','appCategory','advertiserID','appPlatform','appID','positionID', \n",
    "              \n",
    "                #  'app_age_audiences',\n",
    "              ]# 'time' \n",
    "\n",
    "\n",
    "try:\n",
    "    feature_names.remove('clickTime')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_x.append(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_size_dict ={}\n",
    "for feature in cate_features:\n",
    "    all_data[feature] = lbe.fit_transform(all_data[feature])\n",
    "    id_size_dict[feature] = all_data[feature].max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = all_data[all_data.clickDay<27]\n",
    "valid_x = all_data[all_data.clickDay==27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_size_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in cate_features:\n",
    "    id_size_list.append(id_size_dict[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fetches,sess, X=None, y=None,keep_prob=1.0 ):\n",
    "        feed_dict = {}\n",
    "        if type(X) is list:\n",
    "            for i in range(len(X)):\n",
    "                feed_dict[id_inputs[i]] = X[i]\n",
    "                #print(feed_dict[id_inputs[i]])\n",
    "        else:\n",
    "            feed_dict[id_inputs] = X\n",
    "        feed_dict[keep_prob_value] = keep_prob\n",
    "        if y is not None:\n",
    "            feed_dict[label_inputs] = y\n",
    "        return sess.run(fetches, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = cate_features#['appID','gender','connectionType']\n",
    "embedding_size =8\n",
    "id_dtype= tf.int32\n",
    "id_field_num = len(input_feature)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    global_step =  tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "\n",
    "    with tf.name_scope('ID_input'):\n",
    "        id_inputs =[tf.placeholder(id_dtype,shape=[None,]) for i in range(id_field_num)]\n",
    "        label_inputs = tf.placeholder(tf.float32)\n",
    "        keep_prob_value = tf.placeholder(tf.float32,)\n",
    "    \n",
    "    with tf.name_scope('Embedding_matrix'):\n",
    "        embeddings = [tf.Variable(tf.ones([id_size_list[i],embedding_size],)) for i in range(0,id_field_num)]\n",
    "        #embedding1 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        #embedding2 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        #embedding3 = tf.Variable(tf.ones([id_size,embedding_size],))\n",
    "        onehot_embeddings = [tf.Variable(tf.random_normal([id_size_list[i],1])) for i in range(0,id_field_num)]\n",
    "        #onehot_embedding1 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "        #onehot_embedding2 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "        #onehot_embedding3 = tf.Variable(tf.random_normal([id_size,1]))\n",
    "    \n",
    "    with tf.name_scope('Embedding_layer'):\n",
    "        embeds = [tf.nn.embedding_lookup(embeddings[i],id_inputs[i]) for i in range(0,id_field_num)]\n",
    "        #embed1 = tf.nn.embedding_lookup(embedding1,appID)\n",
    "        #embed2 = tf.nn.embedding_lookup(embedding2,genderID)\n",
    "        #embed3 = tf.nn.embedding_lookup(embedding3,connectionTypeID)\n",
    "        \n",
    "        onehot_embeds = [tf.nn.embedding_lookup(onehot_embeddings[i],id_inputs[i]) for i in range(0,id_field_num)]\n",
    "        #onehot_embed1 = tf.nn.embedding_lookup(onehot_embedding1,appID)\n",
    "        #onehot_embed2 = tf.nn.embedding_lookup(onehot_embedding2,genderID)\n",
    "        #onehot_embed3 = tf.nn.embedding_lookup(onehot_embedding3,connectionTypeID)\n",
    "    with tf.name_scope('Hidden_layer'):\n",
    "        nn_input = tf.concat(embeds,0)\n",
    "        fc_1 = tf.contrib.layers.fully_connected(nn_input,200,activation_fn=tf.nn.relu,)\n",
    "        drop_1 = tf.nn.dropout(fc_1,keep_prob_value)\n",
    "        nn_logit = tf.contrib.layers.fully_connected(drop_1,1,activation_fn=None)\n",
    "    with tf.name_scope('FM_layer'):\n",
    "        fm_units = []\n",
    "        embed_list=embeds#[embed1,embed2,embed3]\n",
    "        for i in range(0,id_field_num):\n",
    "            for j in range(i+1,id_field_num):\n",
    "                temp = tf.reduce_sum(tf.multiply(embed_list[i],embed_list[j]),axis=1)\n",
    "                fm_units.append(temp)\n",
    "\n",
    "        sum_fm_units =tf.reduce_sum(fm_units,0)\n",
    "        single_onehot_list = tf.concat(onehot_embeds,1)\n",
    "        sum_linear_units = tf.reduce_sum(single_onehot_list,1)\n",
    "    with tf.name_scope('Output'):\n",
    "        \n",
    "        fm_logits = tf.add(sum_fm_units,sum_linear_units)\n",
    "        logits = fm_logits #+ nn_logit\n",
    "        prob = tf.sigmoid(logits)\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_inputs,logits=logits))\n",
    "        va_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_inputs,logits=logits))\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss,global_step=global_step)\n",
    "    with tf.name_scope('Summary'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_feature = cate_features\n",
    "train_size = train_x.shape[0]\n",
    "valid_size = valid_x.shape[0]\n",
    "train_batch_size = 1024#2560#10240\n",
    "valid_batch_size = 2560#2560\n",
    "train_batch_num = train_size//train_batch_size\n",
    "valid_batch_num = valid_size//valid_batch_size\n",
    "num_round = 100#1000\n",
    "va_input_X = [valid_x[feature].values for feature in input_feature]\n",
    "va_input_y = valid_x['label'].values\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./deepFM_log/',graph=sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_round):\n",
    "        for j in tqdm(range(0,1+train_batch_num)): \n",
    "            X_i = train_x.iloc[j*train_batch_size:min((j+1)*train_batch_size,train_size)]\n",
    "            input_X = [X_i[feature].values for feature in input_feature]#X_i[input_feature]\n",
    "            #print(type(input_X.values))\n",
    "            input_y =X_i['label'].values\n",
    "            #em,nn = sess.run([embeds,nn_input],)\n",
    "            _,l,summary = run([optimizer,loss,summary_op],sess=sess,X = input_X,y = input_y,keep_prob=0.8)\n",
    "            #logit = run([logits],sess=sess,X = input_X,y = input_y)\n",
    "            #print(l)\n",
    "        print(l)\n",
    "        writer.add_summary(summary, i) \n",
    "        va_loss_sum = 0\n",
    "        for j in range(0,1+valid_batch_num):\n",
    "            X_i = valid_x.iloc[j*valid_batch_size:min((j+1)*valid_batch_size,valid_size)]\n",
    "            input_X = [X_i[feature].values for feature in input_feature]#X_i[input_feature]\n",
    "            input_y =X_i['label'].values\n",
    "            va_batch_loss = run([va_loss,],sess=sess,X=input_X,y=input_y,keep_prob=1.0)\n",
    "            #print(va_batch_loss)\n",
    "            va_loss_sum += va_batch_loss[0]\n",
    "        print('epoch {0} va-loss {1}'.format(i,va_loss_sum/valid_size))\n",
    "\n",
    "        saver.save(sess,'save_model',global_step=global_step)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
